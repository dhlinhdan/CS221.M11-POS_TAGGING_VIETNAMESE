{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W_QPjwzf7uF"
      },
      "source": [
        "# ĐỒ ÁN: Xử lý ngôn ngữ tự nhiên\n",
        "Đinh Hoàng Linh Đan     - 19521309\n",
        "Trần Nguyễn Quỳnh Anh   - 19521217  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KZCud4Iqf1Z0"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import unicodedata as ud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import ast\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhdcweSLgBBk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZnAC6tHiTHs"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B9zaNqVHgNn1"
      },
      "outputs": [],
      "source": [
        "sentences = open('C:\\\\Users\\Admin\\\\CS221.M11-POS_TAGGING_VIETNAMESE\\\\Dataset\\\\dataset.txt', encoding='utf-8').readlines()\n",
        "tokenize_sentences = [sentence.split(' ') for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5S5qMoYgNvz",
        "outputId": "a988ae1b-7a8b-4d31-d5bd-700520094084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng câu trong Dataset: 40\n"
          ]
        }
      ],
      "source": [
        "print('Số lượng câu trong Dataset:', len(sentences))\n",
        "#sentences[0:len(sentences)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FBMaGs4j1Xi"
      },
      "source": [
        "# Tách từ bằng Longest Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Zc9yY2RH40q"
      },
      "outputs": [],
      "source": [
        "def syllablize(sentence):\n",
        "    word = '\\w+'\n",
        "    non_word = '[^\\w\\s]'\n",
        "    digits = '\\d+([\\.,_]\\d+)+'\n",
        "    \n",
        "    patterns = []\n",
        "    patterns.extend([word, non_word, digits])\n",
        "    patterns = f\"({'|'.join(patterns)})\"\n",
        "    \n",
        "    sentence = ud.normalize('NFC', sentence)\n",
        "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
        "    return [token[0] for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rRBkmsjIFhAH"
      },
      "outputs": [],
      "source": [
        "def load_n_grams(path):\n",
        "    with open(path, encoding='utf8') as f:\n",
        "        words = f.read()\n",
        "        words = ast.literal_eval(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J80okVzEFhDj"
      },
      "outputs": [],
      "source": [
        "def longest_matching(sentence, two, three):\n",
        "    syllables = syllablize(sentence)\n",
        "    syl_len = len(syllables)\n",
        "    \n",
        "    curr_id = 0\n",
        "    word_list = []\n",
        "    done = False\n",
        "    \n",
        "    while (curr_id < syl_len) and (not done):\n",
        "        curr_word = syllables[curr_id]\n",
        "        if curr_id >= syl_len - 1:\n",
        "            word_list.append(curr_word)\n",
        "            done = True\n",
        "        else:\n",
        "            next_word = syllables[curr_id + 1]\n",
        "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
        "            if curr_id >= (syl_len - 2):\n",
        "                if pair_word in two:\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\n",
        "                    curr_id += 2\n",
        "                else:\n",
        "                    word_list.append(curr_word)\n",
        "                    curr_id += 1\n",
        "            else:\n",
        "                next_next_word = syllables[curr_id + 2]\n",
        "                triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
        "                if triple_word in three:\n",
        "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n",
        "                    curr_id += 3\n",
        "                elif pair_word in two:\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\n",
        "                    curr_id += 2\n",
        "                else:\n",
        "                    word_list.append(curr_word)\n",
        "                    curr_id += 1\n",
        "    return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfPsg6egGxC0",
        "outputId": "9e2755d6-1cb8-440e-a249-2a8eeec3a4c5"
      },
      "outputs": [],
      "source": [
        "two_word = load_n_grams('Resource\\\\two_word.txt')\n",
        "three_word = load_n_grams('Resource\\\\three_word.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUEaPB0VMIZ2",
        "outputId": "6933c805-4f1f-402d-91e5-50ff17960296"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai là hôm_nay .',\n",
              " 'Hãy tách khỏi quá_khứ và chôn vùi nó .',\n",
              " 'Nhưng ta hoàn_toàn không cần phải lo_lắng .']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\\\Longest_Matching_Tokens.txt', 'w', encoding='utf-8') as f:\n",
        "    longest_matching_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = longest_matching(sentence, two_word, three_word)\n",
        "        longest_matching_sentences.append(' '.join(word_list))\n",
        "        for word in word_list: f.write(word + '\\n')\n",
        "        if sentence != sentences[-1]: f.write('\\n')\n",
        "    f.write('\\n')\n",
        "longest_matching_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 156\n"
          ]
        }
      ],
      "source": [
        "count_longest_matching_compounds = 0\n",
        "for sentence in longest_matching_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_longest_matching_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', count_longest_matching_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDPQOWnbC3a"
      },
      "source": [
        "# Tách từ bằng VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LgNrvxAubwHu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vncorenlp in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.3)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (1.26.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (2.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (3.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install vncorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q-GlnKkDbGMd"
      },
      "outputs": [],
      "source": [
        "from vncorenlp import VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = VnCoreNLP(address='http://127.0.0.1', port=9001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eYHwHah0bIje"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai là hôm_nay .',\n",
              " 'Hãy tách khỏi quá_khứ và chôn vùi nó .',\n",
              " 'Nhưng ta hoàn_toàn không cần phải lo_lắng .']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\\\vncore_tokens.txt', 'w', encoding='utf-8') as f:\n",
        "    vncore_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = client.tokenize(sentence)[0]\n",
        "        vncore_sentences.append(' '.join(word_list))\n",
        "        for word in word_list: f.write(word + '\\n')\n",
        "        if sentence != sentences[-1]: f.write('\\n')\n",
        "    f.write('\\n')\n",
        "vncore_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP: 163\n"
          ]
        }
      ],
      "source": [
        "count_vncore_compounds = 0\n",
        "for sentence in vncore_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_vncore_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP:', count_vncore_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdplN6xHuXf"
      },
      "source": [
        "# Tách từ thủ công"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ytWJnGIHgHp",
        "outputId": "fd629fbb-9e77-4eb1-b0a1-8a0bc8e0ed53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai là hôm_nay .',\n",
              " 'Hãy tách khỏi quá_khứ và chôn vùi nó .',\n",
              " 'Nhưng ta hoàn_toàn không cần phải lo_lắng .']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\manual_token.txt', 'r', encoding='utf-8') as f:\n",
        "    manual_tokenize_sentences = []\n",
        "    sentence = ''\n",
        "    for word in f:\n",
        "        if word == '\\n': \n",
        "            manual_tokenize_sentences.append(sentence.strip())\n",
        "            sentence = ''\n",
        "        else: sentence += word.replace('\\n', ' ')\n",
        "manual_tokenize_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ thủ công: 146\n"
          ]
        }
      ],
      "source": [
        "count_manual_tokenize_compounds = 0\n",
        "for sentence in manual_tokenize_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_manual_tokenize_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ thủ công:', count_manual_tokenize_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRduRpBIXzby"
      },
      "source": [
        "# Đánh giá kết quả tách từ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Az5BPXG6X2Oc"
      },
      "outputs": [],
      "source": [
        "def count_correct_words(pred, source, n_grams=3):\n",
        "    pred_words = pred.split()\n",
        "    source_words = source.split()\n",
        "    \n",
        "    total_true, tp = 0, 0\n",
        "    total_errors, fp = 0, 0\n",
        "    \n",
        "    idx = 0\n",
        "    while idx < len(pred_words):\n",
        "        if pred_words[idx] not in source_words[idx:(idx + n_grams)]: \n",
        "            if '_' in pred_words[idx]: fp += 1\n",
        "            del pred_words[idx]\n",
        "            total_errors += 1\n",
        "        else: idx += 1\n",
        "    \n",
        "    idx = 0\n",
        "    while idx < len(source_words):\n",
        "        if source_words[idx] not in pred_words[idx:(idx + n_grams)]: \n",
        "            del source_words[idx]\n",
        "        else: idx += 1\n",
        "    \n",
        "    if len(pred_words) < len(source_words): words = pred_words\n",
        "    else: words = source_words\n",
        "    \n",
        "    for idx in range (len(words)):\n",
        "        if pred_words[idx] == source_words[idx]:\n",
        "            if '_' in pred_words[idx]: tp += 1 \n",
        "            total_true += 1\n",
        "                    \n",
        "    return total_true, total_errors, tp, fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1kxpMgdJYADE"
      },
      "outputs": [],
      "source": [
        "def tokenize_evaluation(pred, source, n_grams=3):\n",
        "    total_true = 0\n",
        "    total_errors = 0\n",
        "    total_words = 0\n",
        "    \n",
        "    pred_tp = 0\n",
        "    pred_fp = 0\n",
        "    \n",
        "    for pred_sentence, source_sentence in zip(pred, source):\n",
        "        total_words += len(source_sentence.split())\n",
        "        if pred_sentence != source_sentence:\n",
        "            true, error, tp, fp = count_correct_words(pred_sentence, source_sentence, n_grams)\n",
        "            total_true += true \n",
        "            total_errors += error\n",
        "            pred_tp += tp\n",
        "            pred_fp += fp\n",
        "        else:\n",
        "            for word in source_sentence.split():\n",
        "                if '_' in word: pred_tp += 1\n",
        "                total_true += 1\n",
        "    return {\n",
        "        'Accuracy': total_true / total_words, \n",
        "        'Precision': pred_tp / (pred_tp + pred_fp),\n",
        "        'Recall': pred_tp / count_manual_tokenize_compounds,\n",
        "        'True Positive': pred_tp, \n",
        "        'False Positive': pred_fp,\n",
        "        'Total True': total_true, \n",
        "        'Total Errors': total_errors\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Longest Matching</th>\n",
              "      <th>Manual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.958549</td>\n",
              "      <td>0.964222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.974359</td>\n",
              "      <td>0.979452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>1.041096</td>\n",
              "      <td>0.979452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True Positive</th>\n",
              "      <td>152</td>\n",
              "      <td>143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False Positive</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total True</th>\n",
              "      <td>555</td>\n",
              "      <td>539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total Errors</th>\n",
              "      <td>34</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Longest Matching    Manual\n",
              "Accuracy               0.958549  0.964222\n",
              "Precision              0.974359  0.979452\n",
              "Recall                 1.041096  0.979452\n",
              "True Positive               152       143\n",
              "False Positive                4         3\n",
              "Total True                  555       539\n",
              "Total Errors                 34        32"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "longest_matching_evaluation = tokenize_evaluation(longest_matching_sentences, vncore_sentences)\n",
        "manual_evaluation = tokenize_evaluation(manual_tokenize_sentences, vncore_sentences)\n",
        "pd.DataFrame(\n",
        "    [longest_matching_evaluation, manual_evaluation], \n",
        "    index = ['Longest Matching', 'Manual']\n",
        ").astype(object).T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZffPxQN2I64b"
      },
      "source": [
        "# Tạo ngữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgQCXhH1I-Ar",
        "outputId": "57e4ca18-5135-491a-b8fc-fd87b64c618e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ: 631\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai\\n', 'là\\n', 'hôm_nay\\n', '.\\n', '\\n']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "manual_tokens = open('Tokens\\\\manual_token.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ:', len(manual_tokens))\n",
        "manual_tokens[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_my9VLvZ994",
        "outputId": "fa7312d5-7a57-4bc2-e240-5c613d80bd57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ: 629\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai\\n', 'là\\n', 'hôm_nay\\n', '.\\n', '\\n']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "longest_matching_tokens = open('Tokens\\\\Longest_Matching_Tokens.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ:', len(longest_matching_tokens))\n",
        "longest_matching_tokens[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up5czJAja1RT"
      },
      "source": [
        "# Gán nhãn bằng VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5NyEJ8vnaGOv"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\label.txt', 'w', encoding='utf-8') as f:\n",
        "    for word in manual_tokens:\n",
        "        word = word.replace('\\n', '')\n",
        "        \n",
        "        if '_' not in word: tag = client.pos_tag(word)\n",
        "        else: tag = client.pos_tag(word.replace('_', ' '))\n",
        "        \n",
        "        if tag == []: f.write('\\n')\n",
        "        else: f.write(f'{word}\\t{tag[0][0][1]}\\n')\n",
        "    f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5cZf7_RnaTZ"
      },
      "source": [
        "# Chia dữ liệu thành tập Train và Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0z4bmr7mned5"
      },
      "outputs": [],
      "source": [
        "label = open('Dataset\\\\label.txt', encoding='utf-8').readlines()\n",
        "new_line_idx = [i for i, item in enumerate(manual_tokens) if item == '\\n']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ar8o-GQYnosB"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\label_train.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[:new_line_idx[32]]: f.write(line)\n",
        "    f.write('\\n')\n",
        "    \n",
        "with open('Dataset\\\\label_test.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[new_line_idx[32] + 1:]: f.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rzSA9kSqWF1X"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\train_words.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[:new_line_idx[32]]: f.write(re.sub('\\t.*', '', line))\n",
        "    f.write('\\n')        \n",
        "    \n",
        "with open('Dataset\\\\test_words.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[new_line_idx[32] + 1:]: f.write(re.sub('\\t.*', '', line))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "80TGiiLmXsAr"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "_rfBzL8lXtzH"
      },
      "outputs": [],
      "source": [
        "def preprocess(vocabs_dict, path):\n",
        "    data = []\n",
        "    file = open(path, encoding='utf-8').readlines()\n",
        "    \n",
        "    for index, word in enumerate(file):\n",
        "        if not (word.lower()).split():\n",
        "            word = '--n--'\n",
        "            data.append(word)\n",
        "            continue\n",
        "        elif (word.lower()).strip() not in vocabs_dict:\n",
        "            word = '--unknow--'\n",
        "            data.append(word)\n",
        "            continue\n",
        "        data.append(word.strip())\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "g9dZJx-wXv8F"
      },
      "outputs": [],
      "source": [
        "def plot_tag_counts(label):\n",
        "    tags = [word_tag.split()[1] for word_tag in label if word_tag.split()]\n",
        "    tag_counts = pd.DataFrame(tags)[0].value_counts()\n",
        "    tag_counts.plot.bar(rot=0, width=0.7, legend=False, figsize=(15, 5))\n",
        "    return pd.DataFrame(tag_counts).T.assign(Total=tag_counts.sum()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wddUxy5LMBSj"
      },
      "source": [
        "# Vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6IIEuypoMDQq"
      },
      "outputs": [],
      "source": [
        "vocabs = open('Resource\\\\vocabs.txt', encoding='utf-8').read().split('\\n')\n",
        "vocabs_dict = {}\n",
        "index = 0\n",
        "\n",
        "for word in sorted(vocabs): \n",
        "    if word not in vocabs_dict: \n",
        "        vocabs_dict[word] = index  \n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHKlzWGxMKcE",
        "outputId": "0cd6fa83-8bdb-4f2b-8e63-a15399dd8730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ vựng: 54820\n",
            ": 0\n",
            "!: 1\n",
            "\": 2\n",
            "#: 3\n",
            "$: 4\n",
            "%: 5\n",
            "&: 6\n",
            "': 7\n",
            "'': 8\n",
            "'40s: 9\n",
            "'60s: 10\n",
            "'70s: 11\n",
            "'80s: 12\n",
            "'86: 13\n",
            "'90s: 14\n",
            "'N: 15\n",
            "'S: 16\n",
            "'d: 17\n",
            "'em: 18\n",
            "'ll: 19\n",
            "'m: 20\n"
          ]
        }
      ],
      "source": [
        "print('Số lượng từ vựng:', len(vocabs_dict.keys()))\n",
        "count = 0\n",
        "\n",
        "for key, value in vocabs_dict.items():\n",
        "    print(f'{key}: {value}')\n",
        "    count += 1\n",
        "    if count > 20: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZtt3zbtoiMw"
      },
      "source": [
        "# Tập Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3ja9apokhI",
        "outputId": "22f18d89-5f51-47a3-ce78-85e60e78ca41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập label_train: 520\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai\\tNp\\n', 'là\\tV\\n', 'hôm_nay\\tN\\n', '.\\tCH\\n', '\\n']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_train = open('Dataset\\\\label_train.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ trong tập label_train:', len(label_train))\n",
        "label_train[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt06J5j0YO8E",
        "outputId": "2cdabf3d-6754-464d-c238-04a8e334e25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập train_words: 520\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai', 'là', 'hôm_nay', '.', '--n--']"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_words = preprocess(vocabs_dict, 'Dataset\\\\train_words.txt')\n",
        "print('Số lượng từ trong tập train_words:', len(train_words))\n",
        "train_words[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqx_IRHj4wh7",
        "outputId": "f1d7e6da-2e7f-4efb-875f-b394b1ad0fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Các từ không nằm trong vocabs: hủy, 1/3, khỏe, Tùy, thư_giãn, "
          ]
        }
      ],
      "source": [
        "print('Các từ không nằm trong vocabs', end=': ')\n",
        "for word_tag, word in zip(label_train, train_words):\n",
        "    if word == '--unknow--': print(word_tag.split()[0], end=', ')\n",
        "#plot_tag_counts(label_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXTx4YZhpXkO"
      },
      "source": [
        "# Tập Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh6gImNhpZvt",
        "outputId": "197fdbe8-f920-4951-c617-0593e7ef7639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập test: 112\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Nếu\\tC\\n', 'chúng_ta\\tP\\n', 'đang\\tR\\n', 'mệt\\tA\\n', ',\\tCH\\n']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_test = open('Dataset\\\\label_test.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ trong tập test:', len(label_test))\n",
        "label_test[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehO92wh75fjy",
        "outputId": "fb4a48ae-8f10-4150-9e29-3fe1717bdd3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập test_words: 112\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Nếu',\n",
              " 'chúng_ta',\n",
              " 'đang',\n",
              " 'mệt',\n",
              " ',',\n",
              " 'cơ_thể',\n",
              " 'sẽ',\n",
              " 'tự',\n",
              " 'bắt',\n",
              " 'ta',\n",
              " 'phải',\n",
              " 'ngủ',\n",
              " ',',\n",
              " 'ngay',\n",
              " 'khi',\n",
              " 'ta',\n",
              " 'đang',\n",
              " 'bước_đi',\n",
              " '.',\n",
              " '--n--']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_words = preprocess(vocabs_dict, 'Dataset\\\\test_words.txt')\n",
        "print('Số lượng từ trong tập test_words:', len(test_words))\n",
        "test_words[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fRm4Hnz5hkX",
        "outputId": "1e8c7e9d-1122-44c8-c560-e7dbef788370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Các từ không nằm trong vocabs: gây_ra, "
          ]
        }
      ],
      "source": [
        "print('Các từ không nằm trong vocabs', end=': ')\n",
        "for word_tag, word in zip(label_test, test_words):\n",
        "    if word == '--unknow--': print(word_tag.split()[0], end=', ')\n",
        "#plot_tag_counts(label_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seperate_word_tag(word_tag, vocabs_dict): \n",
        "    if not word_tag.split():\n",
        "        word = '--n--'\n",
        "        tag = '--s--'\n",
        "    else:\n",
        "        word, tag = word_tag.split()\n",
        "        if word not in vocabs_dict: word = '--unknow--'\n",
        "    return word, tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dictionaries(train_gold, vocab):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    prev_tag = '--s--' \n",
        "    for word_tag in train_gold:\n",
        "        word, tag = seperate_word_tag(word_tag, vocab) \n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        emission_counts[(tag, word)] += 1\n",
        "        tag_counts[tag] += 1\n",
        "        prev_tag = tag\n",
        "    return transition_counts, emission_counts, tag_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số nhãn: 16\n",
            "['--s--', 'A', 'C', 'CH', 'Cc', 'E', 'L', 'M', 'N', 'Nc', 'Np', 'P', 'R', 'T', 'V', 'X']\n"
          ]
        }
      ],
      "source": [
        "transition_counts, emission_counts, tag_counts = create_dictionaries(label_train, vocabs_dict)\n",
        "states = sorted(tag_counts.keys())\n",
        "print('Số nhãn:', len(states))\n",
        "print(states)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transition examples: \n",
            "(('--s--', 'Np'), 6)\n",
            "(('Np', 'V'), 2)\n",
            "(('V', 'N'), 20)\n",
            "(('N', 'CH'), 11)\n",
            "(('CH', '--s--'), 33)\n",
            "(('--s--', 'R'), 2)\n",
            "(('R', 'V'), 25)\n",
            "(('V', 'V'), 51)\n",
            "(('N', 'Cc'), 5)\n",
            "(('Cc', 'V'), 5)\n"
          ]
        }
      ],
      "source": [
        "print(\"Transition examples: \")\n",
        "for example in list(transition_counts.items())[:10]:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emission examples: \n",
            "(('Np', '--unk--'), 6)\n",
            "(('V', 'là'), 8)\n",
            "(('N', 'hôm_nay'), 1)\n",
            "(('CH', '.'), 33)\n",
            "(('--s--', '--n--'), 33)\n",
            "(('R', '--unk--'), 2)\n",
            "(('V', 'tách'), 1)\n",
            "(('V', 'khỏi'), 1)\n",
            "(('N', 'quá_khứ'), 1)\n",
            "(('Cc', 'và'), 11)\n"
          ]
        }
      ],
      "source": [
        "print(\"Emission examples: \")\n",
        "for example in list(emission_counts.items())[:10]:\n",
        "    print (example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbuylDh0DXNm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "RnA5uz2vupNv"
      },
      "outputs": [],
      "source": [
        "def seperate_word_tag(word_tag, vocabs_dict): \n",
        "    if not word_tag.split():\n",
        "        word = '--n--'\n",
        "        tag = '--s--'\n",
        "    else:\n",
        "        word, tag = word_tag.split()\n",
        "        if word not in vocabs_dict: word = '--unknow--'\n",
        "    return word, tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QqXzWBzdDbBF"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(train_gold, vocab):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    prev_tag = '--s--' \n",
        "    for word_tag in train_gold:\n",
        "        word, tag = seperate_word_tag(word_tag, vocab) \n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        emission_counts[(tag, word)] += 1\n",
        "        tag_counts[tag] += 1\n",
        "        prev_tag = tag\n",
        "    return transition_counts, emission_counts, tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "DqnflOJeDcv-",
        "outputId": "917d3047-faab-4dde-b1fc-70557a701325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số nhãn: 16\n",
            "['--s--', 'A', 'C', 'CH', 'Cc', 'E', 'L', 'M', 'N', 'Nc', 'Np', 'P', 'R', 'T', 'V', 'X']\n"
          ]
        }
      ],
      "source": [
        "transition_counts, emission_counts, tag_counts = create_dictionaries(label_train, vocabs_dict)\n",
        "states = sorted(tag_counts.keys())\n",
        "print('Số nhãn:', len(states))\n",
        "print(states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3iuQtDEVu4X"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1psLbjH5Vw7e"
      },
      "outputs": [],
      "source": [
        "def predict_pos(words, gold, emission_counts, vocabs_dict, states):\n",
        "    num_correct = 0\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    for word, gold_tuple in zip(words, gold): \n",
        "        gold_tuple_list = gold_tuple.split()\n",
        "        if len(gold_tuple_list) != 2: continue\n",
        "        else: true_label = gold_tuple_list[1]\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        if word not in vocabs_dict: continue\n",
        "        \n",
        "        for pos in states:\n",
        "            if (pos, word) not in emission_counts: continue\n",
        "            count = emission_counts[(pos, word)]\n",
        "            \n",
        "            if count > count_final:\n",
        "                count_final = count\n",
        "                pos_final = pos\n",
        "                    \n",
        "        if pos_final == true_label: num_correct += 1\n",
        "    accuracy = num_correct / len(gold)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CMtqLDoiVxAn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Độ chính xác trên tập train: 0.8884615384615384\n"
          ]
        }
      ],
      "source": [
        "accuracy = predict_pos(train_words, label_train, emission_counts, vocabs_dict, states)\n",
        "print('Độ chính xác trên tập train:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Độ chính xác trên tập test: 0.5446428571428571\n"
          ]
        }
      ],
      "source": [
        "accuracy = predict_pos(test_words, label_test, emission_counts, vocabs_dict, states)\n",
        "print('Độ chính xác trên tập test:', accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hidden Markov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "    \n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "    \n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_tags):\n",
        "            count = 0\n",
        "            key = (all_tags[i], all_tags[j])\n",
        "            if key in transition_counts: count = transition_counts[key]\n",
        "                \n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "            A[i, j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "    return A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>--s--</th>\n",
              "      <th>A</th>\n",
              "      <th>C</th>\n",
              "      <th>CH</th>\n",
              "      <th>Cc</th>\n",
              "      <th>E</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>Nc</th>\n",
              "      <th>Np</th>\n",
              "      <th>P</th>\n",
              "      <th>R</th>\n",
              "      <th>T</th>\n",
              "      <th>V</th>\n",
              "      <th>X</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>--s--</th>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.121184</td>\n",
              "      <td>0.030319</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.212049</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.181760</td>\n",
              "      <td>0.212049</td>\n",
              "      <td>0.060607</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.212715</td>\n",
              "      <td>0.021291</td>\n",
              "      <td>0.170176</td>\n",
              "      <td>0.063829</td>\n",
              "      <td>0.106368</td>\n",
              "      <td>0.042560</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.085099</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.021291</td>\n",
              "      <td>0.042560</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.233984</td>\n",
              "      <td>0.000021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.076905</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.153734</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.153734</td>\n",
              "      <td>0.307391</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.307391</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CH</th>\n",
              "      <td>0.767180</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.023270</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.023270</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.069765</td>\n",
              "      <td>0.046518</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.069765</td>\n",
              "      <td>0.000023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cc</th>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.181645</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.363199</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.453976</td>\n",
              "      <td>0.000091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          --s--         A         C        CH        Cc         E         L  \\\n",
              "--s--  0.000030  0.000030  0.090895  0.000030  0.000030  0.121184  0.030319   \n",
              "A      0.000021  0.212715  0.021291  0.170176  0.063829  0.106368  0.042560   \n",
              "C      0.000077  0.000077  0.000077  0.000077  0.000077  0.000077  0.076905   \n",
              "CH     0.767180  0.000023  0.023270  0.000023  0.000023  0.000023  0.000023   \n",
              "Cc     0.000091  0.000091  0.000091  0.000091  0.000091  0.000091  0.000091   \n",
              "\n",
              "              M         N        Nc        Np         P         R         T  \\\n",
              "--s--  0.090895  0.212049  0.000030  0.181760  0.212049  0.060607  0.000030   \n",
              "A      0.000021  0.085099  0.000021  0.000021  0.021291  0.042560  0.000021   \n",
              "C      0.000077  0.153734  0.000077  0.000077  0.153734  0.307391  0.000077   \n",
              "CH     0.000023  0.023270  0.000023  0.000023  0.069765  0.046518  0.000023   \n",
              "Cc     0.000091  0.181645  0.000091  0.000091  0.000091  0.363199  0.000091   \n",
              "\n",
              "              V         X  \n",
              "--s--  0.000030  0.000030  \n",
              "A      0.233984  0.000021  \n",
              "C      0.307391  0.000077  \n",
              "CH     0.069765  0.000023  \n",
              "Cc     0.453976  0.000091  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha = 0.001\n",
        "for i in range(len(states)): tag_counts.pop(i, None)\n",
        "\n",
        "m = len(states)\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "df = pd.DataFrame(\n",
        "    A[0:16, 0:16], \n",
        "    index = states[0:16], \n",
        "    columns = states[0:16]\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocabs):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(tag_counts)\n",
        "    num_words = len(vocabs)\n",
        "    \n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    emis_keys = set(list(emission_counts.keys()))\n",
        "    \n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_words):\n",
        "            count = 0\n",
        "            key = (all_tags[i], vocabs[j])\n",
        "            if key in emission_counts.keys(): count = emission_counts[key]\n",
        "                \n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "            B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
        "    return B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_words_lower = []\n",
        "for w in test_words:\n",
        "    test_words_lower.append(w.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_words_lower = []\n",
        "for w in test_words:\n",
        "    train_words_lower.append(w.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nếu</th>\n",
              "      <th>chúng_ta</th>\n",
              "      <th>đang</th>\n",
              "      <th>mệt</th>\n",
              "      <th>,</th>\n",
              "      <th>cơ_thể</th>\n",
              "      <th>sẽ</th>\n",
              "      <th>tự</th>\n",
              "      <th>bắt</th>\n",
              "      <th>ta</th>\n",
              "      <th>...</th>\n",
              "      <th>đùng_đùng</th>\n",
              "      <th>hay</th>\n",
              "      <th>giữa</th>\n",
              "      <th>bom</th>\n",
              "      <th>đạn</th>\n",
              "      <th>hiểm_nguy</th>\n",
              "      <th>của</th>\n",
              "      <th>chiến_tranh</th>\n",
              "      <th>.</th>\n",
              "      <th>--n--</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>--s--</th>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.375780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.014760</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CH</th>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.102239</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.337365</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cc</th>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 112 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            nếu  chúng_ta      đang       mệt         ,    cơ_thể        sẽ  \\\n",
              "--s--  0.000011  0.000011  0.000011  0.000011  0.000011  0.000011  0.000011   \n",
              "A      0.000010  0.000010  0.000010  0.000010  0.000010  0.000010  0.000010   \n",
              "C      0.000015  0.000015  0.000015  0.000015  0.000015  0.000015  0.000015   \n",
              "CH     0.000010  0.000010  0.000010  0.000010  0.102239  0.000010  0.000010   \n",
              "Cc     0.000015  0.000015  0.000015  0.000015  0.000015  0.000015  0.000015   \n",
              "\n",
              "             tự       bắt        ta  ...  đùng_đùng       hay      giữa  \\\n",
              "--s--  0.000011  0.000011  0.000011  ...   0.000011  0.000011  0.000011   \n",
              "A      0.000010  0.000010  0.000010  ...   0.000010  0.000010  0.000010   \n",
              "C      0.000015  0.000015  0.000015  ...   0.000015  0.014760  0.000015   \n",
              "CH     0.000010  0.000010  0.000010  ...   0.000010  0.000010  0.000010   \n",
              "Cc     0.000015  0.000015  0.000015  ...   0.000015  0.000015  0.000015   \n",
              "\n",
              "            bom       đạn  hiểm_nguy       của  chiến_tranh         .  \\\n",
              "--s--  0.000011  0.000011   0.000011  0.000011     0.000011  0.000011   \n",
              "A      0.000010  0.000010   0.000010  0.000010     0.000010  0.000010   \n",
              "C      0.000015  0.000015   0.000015  0.000015     0.000015  0.000015   \n",
              "CH     0.000010  0.000010   0.000010  0.000010     0.000010  0.337365   \n",
              "Cc     0.000015  0.000015   0.000015  0.000015     0.000015  0.000015   \n",
              "\n",
              "          --n--  \n",
              "--s--  0.375780  \n",
              "A      0.000010  \n",
              "C      0.000015  \n",
              "CH     0.000010  \n",
              "Cc     0.000015  \n",
              "\n",
              "[5 rows x 112 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cidx  = test_words_lower\n",
        "rvals = sorted(tag_counts.keys())\n",
        "cols = [vocabs_dict[word] for word in cidx]\n",
        "rows = [states.index(tag) for tag in rvals]\n",
        "\n",
        "for i in range(len(states)): tag_counts.pop(i, None)\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocabs_dict))\n",
        "\n",
        "df = pd.DataFrame(B[np.ix_(rows, cols)], index=rvals, columns=cidx)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array([sublist[1:].tolist() for sublist in A])\n",
        "B = B[1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def viterbi_initialize(states, tag_counts, A, B, corpus, vocabs_dict):\n",
        "    num_tags = len(tag_counts)\n",
        "    s_idx = states.index('--s--')\n",
        "    \n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    \n",
        "    for i in range(num_tags):\n",
        "        if A[s_idx, i - 1] == 0: best_probs[i, 0] = float('-inf')\n",
        "        else: \n",
        "            index = vocabs_dict[corpus[0]]\n",
        "            best_probs[i, 0] = math.log(A[s_idx, i - 1]) + math.log(B[i - 1, index])\n",
        "    return best_probs, best_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_probs_train, best_paths_train = viterbi_initialize(states, tag_counts, A, B, train_words_lower, vocabs_dict)\n",
        "best_probs_test, best_paths_test = viterbi_initialize(states, tag_counts, A, B, test_words_lower, vocabs_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def viterbi_forward(A, B, corpus, best_probs, best_paths, vocabs_dict):\n",
        "    num_tags = best_probs.shape[0] \n",
        "    \n",
        "    for i in range(1, len(corpus)): \n",
        "        if i % 5000 == 0: print(f'Processed {i} words...')\n",
        "            \n",
        "        for j in range(num_tags):\n",
        "            best_prob_i = float('-inf')\n",
        "            best_path_i = None\n",
        "            \n",
        "            for k in range(num_tags):\n",
        "                index = vocabs_dict[corpus[i]]\n",
        "                prob = best_probs[k, i - 1] + math.log(A[k, j - 1]) + math.log(B[j - 1, index])\n",
        "\n",
        "                if prob > best_prob_i:\n",
        "                    best_prob_i = prob\n",
        "                    best_path_i = k\n",
        "                    \n",
        "            best_probs[j, i] = best_prob_i\n",
        "            best_paths[j, i] = best_path_i\n",
        "            \n",
        "    return best_probs, best_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_probs_train[0, 1]: -28.732564428426628\n",
            "best_paths_train[0, 4]: 8\n"
          ]
        }
      ],
      "source": [
        "best_probs_train, best_paths_train = viterbi_forward(A, B, train_words_lower, best_probs_train, best_paths_train, vocabs_dict)\n",
        "print('best_probs_train[0, 1]:', best_probs_train[0, 1]) \n",
        "print('best_paths_train[0, 4]:', best_paths_train[0, 4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_probs_test[0, 1]: -28.732564428426628\n",
            "best_paths_test[0, 4]: 8\n"
          ]
        }
      ],
      "source": [
        "best_probs_test, best_paths_test = viterbi_forward(A, B, test_words_lower, best_probs_test, best_paths_test, vocabs_dict)\n",
        "print('best_probs_test[0, 1]:', best_probs_test[0, 1]) \n",
        "print('best_paths_test[0, 4]:', best_paths_test[0, 4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    m = best_paths.shape[1] \n",
        "    z = [None] * m\n",
        "    pred = [None] * m\n",
        "    \n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    for k in range(num_tags):\n",
        "        if best_probs[k, m - 1] > best_prob_for_last_word:\n",
        "            best_prob_for_last_word = best_probs[k, m - 1]\n",
        "            z[m - 1] = k\n",
        "            \n",
        "    pred[m - 1] = states[z[m - 1]]\n",
        "    for i in range(m - 1, -1, -1):\n",
        "        z[i - 1] = best_paths[z[i], i]\n",
        "        pred[i - 1] = states[z[i - 1]]\n",
        "    return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dự đoán cho test_pred[0:15]:\n",
            "['Nếu', 'chúng_ta', 'đang', 'mệt', ',', 'cơ_thể', 'sẽ', 'tự', 'bắt', 'ta', 'phải', 'ngủ', ',', 'ngay', 'khi']\n",
            "['E', 'P', 'A', 'A', 'CH', 'N', 'R', 'V', 'E', 'P', 'V', 'V', 'CH', 'C', 'N']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_pred = viterbi_backward(best_probs_train, best_paths_train, train_words, states)\n",
        "test_pred = viterbi_backward(best_probs_test, best_paths_test, test_words, states)\n",
        "m = len(test_pred)\n",
        "\n",
        "print('Dự đoán cho test_pred[0:15]:')\n",
        "print(test_words[0:15])\n",
        "print(test_pred[0:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nếu/E chúng_ta/P đang/A mệt/A ,/CH cơ_thể/N sẽ/R tự/V bắt/E ta/P phải/V ngủ/V ,/CH ngay/C khi/N ta/P đang/A bước_đi/A ./CH \n",
            "Khi/N buồn_ngủ/R là/V tự_nhiên/V phải/V ngủ/V ,/CH dù/C ý_chí/V của/E chúng_ta/P có/V mạnh_mẽ/A ./CH \n",
            "Hãy/R làm/V cho/E cơ_thể/N bạn/N mệt/R đến/V mức/C không_thể/R thức/V được/V nữa/A ./CH \n",
            "Lo_lắng/V về/E chứng/V mất_ngủ/V thường/R --unk--/V những/L hậu_quả/V còn/C tệ_hại/P hơn/A bản_thân/N căn_bệnh/N này/P ./CH \n",
            "Nhớ/A rằng/Cc chưa/R có/V ai/P chết/A vì/Cc thiếu/R ngủ/V ./CH \n",
            "Chúng_ta/P có_thể/R nhịn/V ăn/V ,/CH nhịn/P uống/A lâu/A hơn/A là/V nhịn/V ngủ/V ./CH \n",
            "Khi/N hoàn_toàn/A kiệt_sức/V ,/CH con_người/C vẫn/R có_thể/R ngủ/V được/V giữa/V trời/M sấm_sét/E đùng_đùng/P hay/C giữa/R bom/V đạn/V hiểm_nguy/M của/E chiến_tranh/N ./CH \n"
          ]
        }
      ],
      "source": [
        "for word, tag in zip(test_words, test_pred):\n",
        "    if word == '--n--': print()\n",
        "    else: print(f'{word}/{tag}', end=' ')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Đánh giá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def report(pred, gold):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for prediction, word_tag in zip(pred, gold):\n",
        "        word_tag_tuple = word_tag.split()\n",
        "        if len(word_tag_tuple) != 2: continue \n",
        "\n",
        "        word, tag = word_tag_tuple\n",
        "        y_pred.append(prediction)\n",
        "        y_true.append(tag)\n",
        "        \n",
        "    print(classification_report(y_pred, y_true))\n",
        "    return y_pred, y_true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập test:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.50      0.46      0.48        13\n",
            "           C       0.60      0.50      0.55         6\n",
            "          CH       1.00      1.00      1.00        12\n",
            "          Cc       0.00      0.00      0.00         2\n",
            "           E       0.80      0.57      0.67         7\n",
            "           L       1.00      1.00      1.00         1\n",
            "           M       0.00      0.00      0.00         2\n",
            "           N       0.45      1.00      0.62         9\n",
            "          Np       0.00      0.00      0.00         0\n",
            "           P       0.88      0.70      0.78        10\n",
            "           R       0.80      0.67      0.73        12\n",
            "           T       0.00      0.00      0.00         0\n",
            "           V       0.79      0.74      0.77        31\n",
            "\n",
            "    accuracy                           0.70       105\n",
            "   macro avg       0.52      0.51      0.51       105\n",
            "weighted avg       0.72      0.70      0.70       105\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Kết quả của mô hình Hidden Markov kết hợp thuật toán Viterbi trên tập test:\\n')\n",
        "y_pred, y_true_test = report(test_pred, label_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

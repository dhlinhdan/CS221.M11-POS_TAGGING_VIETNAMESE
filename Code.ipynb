{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W_QPjwzf7uF"
      },
      "source": [
        "# ĐỒ ÁN: Xử lý ngôn ngữ tự nhiên\n",
        "Đinh Hoàng Linh Đan     - 19521309\n",
        "Trần Nguyễn Quỳnh Anh   - 19521217  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KZCud4Iqf1Z0"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import unicodedata as ud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import ast\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhdcweSLgBBk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZnAC6tHiTHs"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B9zaNqVHgNn1"
      },
      "outputs": [],
      "source": [
        "sentences = open('C:\\\\Users\\Admin\\\\CS221.M11-POS_TAGGING_VIETNAMESE\\\\Dataset\\\\dataset.txt', encoding='utf-8').readlines()\n",
        "tokenize_sentences = [sentence.split(' ') for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5S5qMoYgNvz",
        "outputId": "a988ae1b-7a8b-4d31-d5bd-700520094084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng câu trong Dataset: 40\n"
          ]
        }
      ],
      "source": [
        "print('Số lượng câu trong Dataset:', len(sentences))\n",
        "#sentences[0:len(sentences)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FBMaGs4j1Xi"
      },
      "source": [
        "# Tách từ bằng Longest Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Zc9yY2RH40q"
      },
      "outputs": [],
      "source": [
        "def syllablize(sentence):\n",
        "    word = '\\w+'\n",
        "    non_word = '[^\\w\\s]'\n",
        "    digits = '\\d+([\\.,_]\\d+)+'\n",
        "    \n",
        "    patterns = []\n",
        "    patterns.extend([word, non_word, digits])\n",
        "    patterns = f\"({'|'.join(patterns)})\"\n",
        "    \n",
        "    sentence = ud.normalize('NFC', sentence)\n",
        "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
        "    return [token[0] for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rRBkmsjIFhAH"
      },
      "outputs": [],
      "source": [
        "def load_n_grams(path):\n",
        "    with open(path, encoding='utf8') as f:\n",
        "        words = f.read()\n",
        "        words = ast.literal_eval(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J80okVzEFhDj"
      },
      "outputs": [],
      "source": [
        "def longest_matching(sentence, two, three):\n",
        "    syllables = syllablize(sentence)\n",
        "    syl_len = len(syllables)\n",
        "    \n",
        "    curr_id = 0\n",
        "    word_list = []\n",
        "    done = False\n",
        "    \n",
        "    while (curr_id < syl_len) and (not done):\n",
        "        curr_word = syllables[curr_id]\n",
        "        if curr_id >= syl_len - 1:\n",
        "            word_list.append(curr_word)\n",
        "            done = True\n",
        "        else:\n",
        "            next_word = syllables[curr_id + 1]\n",
        "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
        "            if curr_id >= (syl_len - 2):\n",
        "                if pair_word in two:\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\n",
        "                    curr_id += 2\n",
        "                else:\n",
        "                    word_list.append(curr_word)\n",
        "                    curr_id += 1\n",
        "            else:\n",
        "                next_next_word = syllables[curr_id + 2]\n",
        "                triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
        "                if triple_word in three:\n",
        "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n",
        "                    curr_id += 3\n",
        "                elif pair_word in two:\n",
        "                    word_list.append('_'.join([curr_word, next_word]))\n",
        "                    curr_id += 2\n",
        "                else:\n",
        "                    word_list.append(curr_word)\n",
        "                    curr_id += 1\n",
        "    return word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfPsg6egGxC0",
        "outputId": "9e2755d6-1cb8-440e-a249-2a8eeec3a4c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hãy', 'tách', 'khỏi', 'quá_khứ', 'và', 'chôn_vùi', 'nó', '.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "two_word = load_n_grams('Resource\\\\two_word.txt')\n",
        "three_word = load_n_grams('Resource\\\\three_word.txt')\n",
        "longest_matching('Hãy tách khỏi quá khứ và chôn vùi nó.', two_word, three_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUEaPB0VMIZ2",
        "outputId": "6933c805-4f1f-402d-91e5-50ff17960296"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai là hôm_nay .',\n",
              " 'Hãy tách khỏi quá_khứ và chôn_vùi nó .',\n",
              " 'Nhưng ta hoàn_toàn không cần phải lo_lắng .']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\\\Longest_Matching_Tokens.txt', 'w', encoding='utf-8') as f:\n",
        "    longest_matching_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = longest_matching(sentence, two_word, three_word)\n",
        "        longest_matching_sentences.append(' '.join(word_list))\n",
        "        for word in word_list: f.write(word + '\\n')\n",
        "        if sentence != sentences[-1]: f.write('\\n')\n",
        "    f.write('\\n')\n",
        "longest_matching_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 166\n"
          ]
        }
      ],
      "source": [
        "count_longest_matching_compounds = 0\n",
        "for sentence in longest_matching_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_longest_matching_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', count_longest_matching_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCDPQOWnbC3a"
      },
      "source": [
        "# Tách từ bằng VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LgNrvxAubwHu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vncorenlp in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.3)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (1.26.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->vncorenlp) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "%pip install vncorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "q-GlnKkDbGMd"
      },
      "outputs": [],
      "source": [
        "from vncorenlp import VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = VnCoreNLP(address='http://127.0.0.1', port=9001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "eYHwHah0bIje"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai là hôm_nay .',\n",
              " 'Hãy tách khỏi quá_khứ và chôn vùi nó .',\n",
              " 'Nhưng ta hoàn_toàn không cần phải lo_lắng .']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\\\vncore_tokens.txt', 'w', encoding='utf-8') as f:\n",
        "    vncore_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = client.tokenize(sentence)[0]\n",
        "        vncore_sentences.append(' '.join(word_list))\n",
        "        for word in word_list: f.write(word + '\\n')\n",
        "        if sentence != sentences[-1]: f.write('\\n')\n",
        "    f.write('\\n')\n",
        "vncore_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP: 162\n"
          ]
        }
      ],
      "source": [
        "count_vncore_compounds = 0\n",
        "for sentence in vncore_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_vncore_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP:', count_vncore_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpdplN6xHuXf"
      },
      "source": [
        "# Tách từ thủ công"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ytWJnGIHgHp",
        "outputId": "fd629fbb-9e77-4eb1-b0a1-8a0bc8e0ed53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Tương_lai  là  hôm_nay .',\n",
              " 'Hãy  tách  khỏi  quá_khứ  và  chôn_vùi  nó .',\n",
              " 'Nhưng  ta  hoàn_toàn  không  cần  phải  lo_lắng .']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('Tokens\\manual_token.txt', 'r', encoding='utf-8') as f:\n",
        "    manual_tokenize_sentences = []\n",
        "    sentence = ''\n",
        "    for word in f:\n",
        "        if word == '\\n': \n",
        "            manual_tokenize_sentences.append(sentence.strip())\n",
        "            sentence = ''\n",
        "        else: sentence += word.replace('\\n', ' ')\n",
        "manual_tokenize_sentences[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ ghép khi tách từ thủ công: 171\n"
          ]
        }
      ],
      "source": [
        "count_manual_tokenize_compounds = 0\n",
        "for sentence in manual_tokenize_sentences:\n",
        "    for word in sentence.split():\n",
        "        if '_' in word: count_manual_tokenize_compounds += 1\n",
        "print('Số lượng từ ghép khi tách từ thủ công:', count_manual_tokenize_compounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRduRpBIXzby"
      },
      "source": [
        "# Đánh giá kết quả tách từ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Az5BPXG6X2Oc"
      },
      "outputs": [],
      "source": [
        "def count_correct_words(pred, source, n_grams=3):\n",
        "    pred_words = pred.split()\n",
        "    source_words = source.split()\n",
        "    \n",
        "    total_true, tp = 0, 0\n",
        "    total_errors, fp = 0, 0\n",
        "    \n",
        "    idx = 0\n",
        "    while idx < len(pred_words):\n",
        "        if pred_words[idx] not in source_words[idx:(idx + n_grams)]: \n",
        "            if '_' in pred_words[idx]: fp += 1\n",
        "            del pred_words[idx]\n",
        "            total_errors += 1\n",
        "        else: idx += 1\n",
        "    \n",
        "    idx = 0\n",
        "    while idx < len(source_words):\n",
        "        if source_words[idx] not in pred_words[idx:(idx + n_grams)]: \n",
        "            del source_words[idx]\n",
        "        else: idx += 1\n",
        "    \n",
        "    if len(pred_words) < len(source_words): words = pred_words\n",
        "    else: words = source_words\n",
        "    \n",
        "    for idx in range (len(words)):\n",
        "        if pred_words[idx] == source_words[idx]:\n",
        "            if '_' in pred_words[idx]: tp += 1 \n",
        "            total_true += 1\n",
        "                    \n",
        "    return total_true, total_errors, tp, fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1kxpMgdJYADE"
      },
      "outputs": [],
      "source": [
        "def tokenize_evaluation(pred, source, n_grams=3):\n",
        "    total_true = 0\n",
        "    total_errors = 0\n",
        "    total_words = 0\n",
        "    \n",
        "    pred_tp = 0\n",
        "    pred_fp = 0\n",
        "    \n",
        "    for pred_sentence, source_sentence in zip(pred, source):\n",
        "        total_words += len(source_sentence.split())\n",
        "        if pred_sentence != source_sentence:\n",
        "            true, error, tp, fp = count_correct_words(pred_sentence, source_sentence, n_grams)\n",
        "            total_true += true \n",
        "            total_errors += error\n",
        "            pred_tp += tp\n",
        "            pred_fp += fp\n",
        "        else:\n",
        "            for word in source_sentence.split():\n",
        "                if '_' in word: pred_tp += 1\n",
        "                total_true += 1\n",
        "    return {\n",
        "        'Accuracy': total_true / total_words, \n",
        "        'Precision': pred_tp / (pred_tp + pred_fp),\n",
        "        'Recall': pred_tp / count_manual_tokenize_compounds,\n",
        "        'True Positive': pred_tp, \n",
        "        'False Positive': pred_fp,\n",
        "        'Total True': total_true, \n",
        "        'Total Errors': total_errors\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Longest Matching</th>\n",
              "      <th>VnCoreNLP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.079855</td>\n",
              "      <td>0.07804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Precision</th>\n",
              "      <td>0.090361</td>\n",
              "      <td>0.08642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recall</th>\n",
              "      <td>0.087719</td>\n",
              "      <td>0.081871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True Positive</th>\n",
              "      <td>15</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False Positive</th>\n",
              "      <td>151</td>\n",
              "      <td>148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total True</th>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Total Errors</th>\n",
              "      <td>535</td>\n",
              "      <td>537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Longest Matching VnCoreNLP\n",
              "Accuracy               0.079855   0.07804\n",
              "Precision              0.090361   0.08642\n",
              "Recall                 0.087719  0.081871\n",
              "True Positive                15        14\n",
              "False Positive              151       148\n",
              "Total True                   44        43\n",
              "Total Errors                535       537"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "longest_matching_evaluation = tokenize_evaluation(longest_matching_sentences, manual_tokenize_sentences)\n",
        "vncore_evaluation = tokenize_evaluation(vncore_sentences, manual_tokenize_sentences)\n",
        "pd.DataFrame(\n",
        "    [longest_matching_evaluation, vncore_evaluation], \n",
        "    index = ['Longest Matching', 'VnCoreNLP']\n",
        ").astype(object).T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZffPxQN2I64b"
      },
      "source": [
        "# Tạo ngữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgQCXhH1I-Ar",
        "outputId": "57e4ca18-5135-491a-b8fc-fd87b64c618e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ: 620\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai \\n', 'là \\n', 'hôm_nay\\n', '.\\n', '\\n']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "manual_tokens = open('Tokens\\\\manual_token.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ:', len(manual_tokens))\n",
        "manual_tokens[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_my9VLvZ994",
        "outputId": "fa7312d5-7a57-4bc2-e240-5c613d80bd57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ: 619\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai\\n', 'là\\n', 'hôm_nay\\n', '.\\n', '\\n']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "longest_matching_tokens = open('Tokens\\\\Longest_Matching_Tokens.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ:', len(longest_matching_tokens))\n",
        "longest_matching_tokens[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up5czJAja1RT"
      },
      "source": [
        "# Gán nhãn bằng VnCoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5NyEJ8vnaGOv"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\label.txt', 'w', encoding='utf-8') as f:\n",
        "    for word in manual_tokens:\n",
        "        word = word.replace('\\n', '')\n",
        "        \n",
        "        if '_' not in word: tag = client.pos_tag(word)\n",
        "        else: tag = client.pos_tag(word.replace('_', ' '))\n",
        "        \n",
        "        if tag == []: f.write('\\n')\n",
        "        else: f.write(f'{word}\\t{tag[0][0][1]}\\n')\n",
        "    f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5cZf7_RnaTZ"
      },
      "source": [
        "# Chia dữ liệu thành tập Train và Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0z4bmr7mned5"
      },
      "outputs": [],
      "source": [
        "label = open('Dataset\\\\label.txt', encoding='utf-8').readlines()\n",
        "new_line_idx = [i for i, item in enumerate(manual_tokens) if item == '\\n']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ar8o-GQYnosB"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\label_train.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[:new_line_idx[32]]: f.write(line)\n",
        "    f.write('\\n')\n",
        "    \n",
        "with open('Dataset\\\\label_test.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[new_line_idx[32] + 1:]: f.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rzSA9kSqWF1X"
      },
      "outputs": [],
      "source": [
        "with open('Dataset\\\\train_words.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[:new_line_idx[32]]: f.write(re.sub('\\t.*', '', line))\n",
        "    f.write('\\n')        \n",
        "    \n",
        "with open('Dataset\\\\test_words.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in label[new_line_idx[32] + 1:]: f.write(re.sub('\\t.*', '', line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80TGiiLmXsAr"
      },
      "source": [
        "# Đọc dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_rfBzL8lXtzH"
      },
      "outputs": [],
      "source": [
        "def preprocess(vocabs_dict, path):\n",
        "    data = []\n",
        "    file = open(path, encoding='utf-8').readlines()\n",
        "    \n",
        "    for index, word in enumerate(file):\n",
        "        if not (word.lower()).split():\n",
        "            word = '--n--'\n",
        "            data.append(word)\n",
        "            continue\n",
        "        elif (word.lower()).strip() not in vocabs_dict:\n",
        "            word = '--unk--'\n",
        "            data.append(word)\n",
        "            continue\n",
        "        data.append(word.strip())\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g9dZJx-wXv8F"
      },
      "outputs": [],
      "source": [
        "def plot_tag_counts(label):\n",
        "    tags = [word_tag.split()[1] for word_tag in label if word_tag.split()]\n",
        "    tag_counts = pd.DataFrame(tags)[0].value_counts()\n",
        "    tag_counts.plot.bar(rot=0, width=0.7, legend=False, figsize=(15, 5))\n",
        "    return pd.DataFrame(tag_counts).T.assign(Total=tag_counts.sum()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wddUxy5LMBSj"
      },
      "source": [
        "# Vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6IIEuypoMDQq"
      },
      "outputs": [],
      "source": [
        "vocabs = open('Resource\\\\vocabs.txt', encoding='utf-8').read().split('\\n')\n",
        "vocabs_dict = {}\n",
        "index = 0\n",
        "\n",
        "for word in sorted(vocabs): \n",
        "    if word not in vocabs_dict: \n",
        "        vocabs_dict[word] = index  \n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHKlzWGxMKcE",
        "outputId": "0cd6fa83-8bdb-4f2b-8e63-a15399dd8730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ vựng: 54817\n",
            "!: 0\n",
            "\": 1\n",
            "#: 2\n",
            "$: 3\n",
            "%: 4\n",
            "&: 5\n",
            "': 6\n",
            "'': 7\n",
            "'40s: 8\n",
            "'60s: 9\n",
            "'70s: 10\n",
            "'80s: 11\n",
            "'86: 12\n",
            "'90s: 13\n",
            "'N: 14\n",
            "'S: 15\n",
            "'d: 16\n",
            "'em: 17\n",
            "'ll: 18\n",
            "'m: 19\n",
            "'n': 20\n"
          ]
        }
      ],
      "source": [
        "print('Số lượng từ vựng:', len(vocabs_dict.keys()))\n",
        "count = 0\n",
        "\n",
        "for key, value in vocabs_dict.items():\n",
        "    print(f'{key}: {value}')\n",
        "    count += 1\n",
        "    if count > 20: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZtt3zbtoiMw"
      },
      "source": [
        "# Tập Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3ja9apokhI",
        "outputId": "22f18d89-5f51-47a3-ce78-85e60e78ca41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập label_train: 468\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai \\tNp\\n', 'là \\tV\\n', 'hôm_nay\\tN\\n', '.\\tCH\\n', '\\n']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_train = open('Dataset\\\\label_train.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ trong tập label_train:', len(label_train))\n",
        "label_train[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt06J5j0YO8E",
        "outputId": "2cdabf3d-6754-464d-c238-04a8e334e25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập train_words: 468\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Tương_lai', 'là', 'hôm_nay', '.', '--n--']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_words = preprocess(vocabs_dict, 'Dataset\\\\train_words.txt')\n",
        "print('Số lượng từ trong tập train_words:', len(train_words))\n",
        "train_words[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqx_IRHj4wh7",
        "outputId": "f1d7e6da-2e7f-4efb-875f-b394b1ad0fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Các từ không nằm trong vocabs: chôn_vùi, nỗi_buồn, mệnh_mông, bất, như_thế, hủy_hoại, xảy_ra, tìm_ra, không_được, chuyện_gì, xảy_ra, tạo_ra, đáng_ngạc_nhiên, 45kg, Người_chồng, thứ_nhất, sau_khi, sức_lực., Hiếm_khi, gì_đó, gây_ra, buồn_chán, nể_phục, không_hề, không_biết, 1/3, người_lính, leo_núi, khỏe_mạnh, đó., Tùy, lo, đem_lại, "
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V</th>\n",
              "      <th>N</th>\n",
              "      <th>A</th>\n",
              "      <th>CH</th>\n",
              "      <th>P</th>\n",
              "      <th>R</th>\n",
              "      <th>E</th>\n",
              "      <th>L</th>\n",
              "      <th>C</th>\n",
              "      <th>Cc</th>\n",
              "      <th>M</th>\n",
              "      <th>Np</th>\n",
              "      <th>Nc</th>\n",
              "      <th>X</th>\n",
              "      <th>tận</th>\n",
              "      <th>T</th>\n",
              "      <th>lắng</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120</td>\n",
              "      <td>71</td>\n",
              "      <td>44</td>\n",
              "      <td>39</td>\n",
              "      <td>36</td>\n",
              "      <td>33</td>\n",
              "      <td>32</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     V   N   A  CH   P   R   E   L   C  Cc  M  Np  Nc  X  tận  T  lắng  Total\n",
              "0  120  71  44  39  36  33  32  16  11  10  8   6   2  2    1  1     1    433"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('Các từ không nằm trong vocabs', end=': ')\n",
        "for word_tag, word in zip(label_train, train_words):\n",
        "    if word == '--unk--': print(word_tag.split()[0], end=', ')\n",
        "plot_tag_counts(label_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXTx4YZhpXkO"
      },
      "source": [
        "# Tập Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh6gImNhpZvt",
        "outputId": "197fdbe8-f920-4951-c617-0593e7ef7639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập test: 153\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Nếu \\tC\\n', 'bạn \\tN\\n', 'không \\tR\\n', 'có \\tV\\n', 'một \\tM\\n']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_test = open('Dataset\\\\label_test.txt', encoding='utf-8').readlines()\n",
        "print('Số lượng từ trong tập test:', len(label_test))\n",
        "label_test[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehO92wh75fjy",
        "outputId": "fb4a48ae-8f10-4150-9e29-3fe1717bdd3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số lượng từ trong tập test_words: 153\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Nếu', 'bạn', 'không', 'có', 'một']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_words = preprocess(vocabs_dict, 'Dataset\\\\test_words.txt')\n",
        "print('Số lượng từ trong tập test_words:', len(test_words))\n",
        "test_words[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fRm4Hnz5hkX",
        "outputId": "1e8c7e9d-1122-44c8-c560-e7dbef788370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Các từ không nằm trong vocabs: tín, vất, thư, biện, thể, tốt_nhất, gây, có, "
          ]
        }
      ],
      "source": [
        "print('Các từ không nằm trong vocabs', end=': ')\n",
        "for word_tag, word in zip(label_test, test_words):\n",
        "    if word == '--unk--': print(word_tag.split()[0], end=', ')\n",
        "#plot_tag_counts(label_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seperate_word_tag(word_tag, vocabs_dict): \n",
        "    if not word_tag.split():\n",
        "        word = '--n--'\n",
        "        tag = '--s--'\n",
        "    else:\n",
        "        word, tag = word_tag.split()\n",
        "        if word not in vocabs_dict: word = '--unk--'\n",
        "    return word, tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dictionaries(train_gold, vocab):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    prev_tag = '--s--' \n",
        "    for word_tag in train_gold:\n",
        "        word, tag = seperate_word_tag(word_tag, vocab) \n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        emission_counts[(tag, word)] += 1\n",
        "        tag_counts[tag] += 1\n",
        "        prev_tag = tag\n",
        "    return transition_counts, emission_counts, tag_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13632/2153056028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransition_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memission_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dictionaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabs_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Số nhãn:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13632/325187851.py\u001b[0m in \u001b[0;36mcreate_dictionaries\u001b[1;34m(train_gold, vocab)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprev_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'--s--'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_gold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseperate_word_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtransition_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13632/1496156456.py\u001b[0m in \u001b[0;36mseperate_word_tag\u001b[1;34m(word_tag, vocabs_dict)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'--s--'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabs_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'--unk--'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "transition_counts, emission_counts, tag_counts = create_dictionaries(label_train, vocabs_dict)\n",
        "states = sorted(tag_counts.keys())\n",
        "print('Số nhãn:', len(states))\n",
        "print(states)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transition examples: \n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'transition_counts' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13632/3247437004.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Transition examples: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'transition_counts' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Transition examples: \")\n",
        "for example in list(transition_counts.items())[:3]:\n",
        "    print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Emission examples: \")\n",
        "for example in list(emission_counts.items())[:3]:\n",
        "    print (example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbuylDh0DXNm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RnA5uz2vupNv"
      },
      "outputs": [],
      "source": [
        "def seperate_word_tag(word_tag, vocabs_dict): \n",
        "    if not word_tag.split():\n",
        "        word = '--n--'\n",
        "        tag = '--s--'\n",
        "    else:\n",
        "        word, tag = word_tag.split()\n",
        "        if word not in vocabs_dict: word = '--unk--'\n",
        "    return word, tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "QqXzWBzdDbBF"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(train_gold, vocab):\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    prev_tag = '--s--' \n",
        "    for word_tag in train_gold:\n",
        "        word, tag = seperate_word_tag(word_tag, vocab) \n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        emission_counts[(tag, word)] += 1\n",
        "        tag_counts[tag] += 1\n",
        "        prev_tag = tag\n",
        "    return transition_counts, emission_counts, tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "DqnflOJeDcv-",
        "outputId": "917d3047-faab-4dde-b1fc-70557a701325"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\4.1\\CS221.N11\\CS221.M11-POS_TAGGING_VIETNAMESE\\CS221.M11-POS_TAGGING_VIETNAMESE\\Code.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transition_counts, emission_counts, tag_counts \u001b[39m=\u001b[39m create_dictionaries(label_train, vocabs_dict)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m states \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(tag_counts\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSố nhãn:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(states))\n",
            "\u001b[1;32md:\\4.1\\CS221.N11\\CS221.M11-POS_TAGGING_VIETNAMESE\\CS221.M11-POS_TAGGING_VIETNAMESE\\Code.ipynb Cell 50\u001b[0m in \u001b[0;36mcreate_dictionaries\u001b[1;34m(train_gold, vocab)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prev_tag \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m--s--\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m word_tag \u001b[39min\u001b[39;00m train_gold:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     word, tag \u001b[39m=\u001b[39m seperate_word_tag(word_tag, vocab) \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     transition_counts[(prev_tag, tag)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     emission_counts[(tag, word)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;32md:\\4.1\\CS221.N11\\CS221.M11-POS_TAGGING_VIETNAMESE\\CS221.M11-POS_TAGGING_VIETNAMESE\\Code.ipynb Cell 50\u001b[0m in \u001b[0;36mseperate_word_tag\u001b[1;34m(word_tag, vocabs_dict)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tag \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m--s--\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     word, tag \u001b[39m=\u001b[39m word_tag\u001b[39m.\u001b[39msplit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocabs_dict: word \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m--unk--\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/4.1/CS221.N11/CS221.M11-POS_TAGGING_VIETNAMESE/CS221.M11-POS_TAGGING_VIETNAMESE/Code.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m word, tag\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "transition_counts, emission_counts, tag_counts = create_dictionaries(label_train, vocabs_dict)\n",
        "states = sorted(tag_counts.keys())\n",
        "print('Số nhãn:', len(states))\n",
        "print(states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3iuQtDEVu4X"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1psLbjH5Vw7e"
      },
      "outputs": [],
      "source": [
        "def predict_pos(words, gold, emission_counts, vocabs_dict, states):\n",
        "    num_correct = 0\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    for word, gold_tuple in zip(words, gold): \n",
        "        gold_tuple_list = gold_tuple.split()\n",
        "        if len(gold_tuple_list) != 2: continue\n",
        "        else: true_label = gold_tuple_list[1]\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        if word not in vocabs_dict: continue\n",
        "        \n",
        "        for pos in states:\n",
        "            if (pos, word) not in emission_counts: continue\n",
        "            count = emission_counts[(pos, word)]\n",
        "            \n",
        "            if count > count_final:\n",
        "                count_final = count\n",
        "                pos_final = pos\n",
        "                    \n",
        "        if pos_final == true_label: num_correct += 1\n",
        "    accuracy = num_correct / len(gold)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMtqLDoiVxAn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
